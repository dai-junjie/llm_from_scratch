{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 6104,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00655307994757536,
      "grad_norm": 7.40625,
      "learning_rate": 1.9937745740498036e-05,
      "loss": 1.5318,
      "step": 20
    },
    {
      "epoch": 0.01310615989515072,
      "grad_norm": 6.84375,
      "learning_rate": 1.9872214941022282e-05,
      "loss": 1.5937,
      "step": 40
    },
    {
      "epoch": 0.019659239842726082,
      "grad_norm": 8.25,
      "learning_rate": 1.9806684141546528e-05,
      "loss": 1.5775,
      "step": 60
    },
    {
      "epoch": 0.02621231979030144,
      "grad_norm": 6.625,
      "learning_rate": 1.9741153342070774e-05,
      "loss": 1.5887,
      "step": 80
    },
    {
      "epoch": 0.0327653997378768,
      "grad_norm": 8.1875,
      "learning_rate": 1.967562254259502e-05,
      "loss": 1.6771,
      "step": 100
    },
    {
      "epoch": 0.039318479685452164,
      "grad_norm": 6.78125,
      "learning_rate": 1.961009174311927e-05,
      "loss": 1.6208,
      "step": 120
    },
    {
      "epoch": 0.045871559633027525,
      "grad_norm": 6.8125,
      "learning_rate": 1.9544560943643512e-05,
      "loss": 1.5986,
      "step": 140
    },
    {
      "epoch": 0.05242463958060288,
      "grad_norm": 5.5,
      "learning_rate": 1.9479030144167762e-05,
      "loss": 1.5756,
      "step": 160
    },
    {
      "epoch": 0.05897771952817824,
      "grad_norm": 6.53125,
      "learning_rate": 1.9413499344692005e-05,
      "loss": 1.6185,
      "step": 180
    },
    {
      "epoch": 0.0655307994757536,
      "grad_norm": 7.5625,
      "learning_rate": 1.9347968545216254e-05,
      "loss": 1.6576,
      "step": 200
    },
    {
      "epoch": 0.07208387942332896,
      "grad_norm": 7.0625,
      "learning_rate": 1.92824377457405e-05,
      "loss": 1.5632,
      "step": 220
    },
    {
      "epoch": 0.07863695937090433,
      "grad_norm": 7.5625,
      "learning_rate": 1.9216906946264746e-05,
      "loss": 1.6131,
      "step": 240
    },
    {
      "epoch": 0.08519003931847968,
      "grad_norm": 7.1875,
      "learning_rate": 1.9151376146788992e-05,
      "loss": 1.5961,
      "step": 260
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 6.65625,
      "learning_rate": 1.908584534731324e-05,
      "loss": 1.6266,
      "step": 280
    },
    {
      "epoch": 0.0982961992136304,
      "grad_norm": 6.8125,
      "learning_rate": 1.9020314547837485e-05,
      "loss": 1.6174,
      "step": 300
    },
    {
      "epoch": 0.10484927916120576,
      "grad_norm": 5.9375,
      "learning_rate": 1.895478374836173e-05,
      "loss": 1.6495,
      "step": 320
    },
    {
      "epoch": 0.11140235910878113,
      "grad_norm": 6.4375,
      "learning_rate": 1.888925294888598e-05,
      "loss": 1.7649,
      "step": 340
    },
    {
      "epoch": 0.11795543905635648,
      "grad_norm": 6.71875,
      "learning_rate": 1.8823722149410223e-05,
      "loss": 1.7851,
      "step": 360
    },
    {
      "epoch": 0.12450851900393185,
      "grad_norm": 6.6875,
      "learning_rate": 1.8758191349934472e-05,
      "loss": 1.7413,
      "step": 380
    },
    {
      "epoch": 0.1310615989515072,
      "grad_norm": 7.53125,
      "learning_rate": 1.8692660550458715e-05,
      "loss": 1.8169,
      "step": 400
    },
    {
      "epoch": 0.13761467889908258,
      "grad_norm": 7.25,
      "learning_rate": 1.8627129750982964e-05,
      "loss": 1.7302,
      "step": 420
    },
    {
      "epoch": 0.14416775884665792,
      "grad_norm": 7.28125,
      "learning_rate": 1.856159895150721e-05,
      "loss": 1.736,
      "step": 440
    },
    {
      "epoch": 0.15072083879423329,
      "grad_norm": 6.96875,
      "learning_rate": 1.8496068152031457e-05,
      "loss": 1.7594,
      "step": 460
    },
    {
      "epoch": 0.15727391874180865,
      "grad_norm": 7.78125,
      "learning_rate": 1.8430537352555703e-05,
      "loss": 1.7984,
      "step": 480
    },
    {
      "epoch": 0.16382699868938402,
      "grad_norm": 7.875,
      "learning_rate": 1.836500655307995e-05,
      "loss": 1.7581,
      "step": 500
    },
    {
      "epoch": 0.17038007863695936,
      "grad_norm": 6.28125,
      "learning_rate": 1.8299475753604195e-05,
      "loss": 1.7692,
      "step": 520
    },
    {
      "epoch": 0.17693315858453473,
      "grad_norm": 7.21875,
      "learning_rate": 1.823394495412844e-05,
      "loss": 1.7702,
      "step": 540
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 6.875,
      "learning_rate": 1.816841415465269e-05,
      "loss": 1.7492,
      "step": 560
    },
    {
      "epoch": 0.19003931847968544,
      "grad_norm": 6.5625,
      "learning_rate": 1.8102883355176933e-05,
      "loss": 1.742,
      "step": 580
    },
    {
      "epoch": 0.1965923984272608,
      "grad_norm": 7.90625,
      "learning_rate": 1.8037352555701183e-05,
      "loss": 1.7889,
      "step": 600
    },
    {
      "epoch": 0.20314547837483618,
      "grad_norm": 6.5625,
      "learning_rate": 1.797182175622543e-05,
      "loss": 1.7561,
      "step": 620
    },
    {
      "epoch": 0.20969855832241152,
      "grad_norm": 7.875,
      "learning_rate": 1.7906290956749675e-05,
      "loss": 1.779,
      "step": 640
    },
    {
      "epoch": 0.2162516382699869,
      "grad_norm": 7.0,
      "learning_rate": 1.784076015727392e-05,
      "loss": 1.7865,
      "step": 660
    },
    {
      "epoch": 0.22280471821756226,
      "grad_norm": 6.90625,
      "learning_rate": 1.7775229357798167e-05,
      "loss": 1.7866,
      "step": 680
    },
    {
      "epoch": 0.22935779816513763,
      "grad_norm": 7.03125,
      "learning_rate": 1.7709698558322413e-05,
      "loss": 1.7896,
      "step": 700
    },
    {
      "epoch": 0.23591087811271297,
      "grad_norm": 7.09375,
      "learning_rate": 1.764416775884666e-05,
      "loss": 1.8054,
      "step": 720
    },
    {
      "epoch": 0.24246395806028834,
      "grad_norm": 7.65625,
      "learning_rate": 1.7578636959370905e-05,
      "loss": 1.8092,
      "step": 740
    },
    {
      "epoch": 0.2490170380078637,
      "grad_norm": 7.53125,
      "learning_rate": 1.751310615989515e-05,
      "loss": 1.7791,
      "step": 760
    },
    {
      "epoch": 0.25557011795543905,
      "grad_norm": 6.8125,
      "learning_rate": 1.7447575360419397e-05,
      "loss": 1.7246,
      "step": 780
    },
    {
      "epoch": 0.2621231979030144,
      "grad_norm": 6.78125,
      "learning_rate": 1.7382044560943643e-05,
      "loss": 1.7209,
      "step": 800
    },
    {
      "epoch": 0.2686762778505898,
      "grad_norm": 7.0625,
      "learning_rate": 1.7316513761467893e-05,
      "loss": 1.7189,
      "step": 820
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 6.65625,
      "learning_rate": 1.725098296199214e-05,
      "loss": 1.7524,
      "step": 840
    },
    {
      "epoch": 0.2817824377457405,
      "grad_norm": 6.59375,
      "learning_rate": 1.7185452162516385e-05,
      "loss": 1.7494,
      "step": 860
    },
    {
      "epoch": 0.28833551769331583,
      "grad_norm": 6.40625,
      "learning_rate": 1.711992136304063e-05,
      "loss": 1.7885,
      "step": 880
    },
    {
      "epoch": 0.2948885976408912,
      "grad_norm": 7.59375,
      "learning_rate": 1.7054390563564877e-05,
      "loss": 1.8056,
      "step": 900
    },
    {
      "epoch": 0.30144167758846657,
      "grad_norm": 7.28125,
      "learning_rate": 1.6988859764089123e-05,
      "loss": 1.6954,
      "step": 920
    },
    {
      "epoch": 0.30799475753604194,
      "grad_norm": 6.46875,
      "learning_rate": 1.692332896461337e-05,
      "loss": 1.7269,
      "step": 940
    },
    {
      "epoch": 0.3145478374836173,
      "grad_norm": 6.25,
      "learning_rate": 1.6857798165137616e-05,
      "loss": 1.7395,
      "step": 960
    },
    {
      "epoch": 0.3211009174311927,
      "grad_norm": 6.09375,
      "learning_rate": 1.679226736566186e-05,
      "loss": 1.712,
      "step": 980
    },
    {
      "epoch": 0.32765399737876805,
      "grad_norm": 6.6875,
      "learning_rate": 1.6726736566186108e-05,
      "loss": 1.7296,
      "step": 1000
    },
    {
      "epoch": 0.33420707732634336,
      "grad_norm": 6.0625,
      "learning_rate": 1.6661205766710357e-05,
      "loss": 1.7734,
      "step": 1020
    },
    {
      "epoch": 0.34076015727391873,
      "grad_norm": 5.96875,
      "learning_rate": 1.65956749672346e-05,
      "loss": 1.7459,
      "step": 1040
    },
    {
      "epoch": 0.3473132372214941,
      "grad_norm": 8.3125,
      "learning_rate": 1.653014416775885e-05,
      "loss": 1.7913,
      "step": 1060
    },
    {
      "epoch": 0.35386631716906947,
      "grad_norm": 6.65625,
      "learning_rate": 1.6464613368283092e-05,
      "loss": 1.772,
      "step": 1080
    },
    {
      "epoch": 0.36041939711664484,
      "grad_norm": 5.46875,
      "learning_rate": 1.639908256880734e-05,
      "loss": 1.7567,
      "step": 1100
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 6.65625,
      "learning_rate": 1.6333551769331588e-05,
      "loss": 1.7579,
      "step": 1120
    },
    {
      "epoch": 0.3735255570117955,
      "grad_norm": 7.1875,
      "learning_rate": 1.6268020969855834e-05,
      "loss": 1.7678,
      "step": 1140
    },
    {
      "epoch": 0.3800786369593709,
      "grad_norm": 7.28125,
      "learning_rate": 1.620249017038008e-05,
      "loss": 1.7853,
      "step": 1160
    },
    {
      "epoch": 0.38663171690694625,
      "grad_norm": 6.59375,
      "learning_rate": 1.6136959370904326e-05,
      "loss": 1.7003,
      "step": 1180
    },
    {
      "epoch": 0.3931847968545216,
      "grad_norm": 6.28125,
      "learning_rate": 1.6071428571428572e-05,
      "loss": 1.7568,
      "step": 1200
    },
    {
      "epoch": 0.399737876802097,
      "grad_norm": 6.875,
      "learning_rate": 1.6005897771952818e-05,
      "loss": 1.6939,
      "step": 1220
    },
    {
      "epoch": 0.40629095674967236,
      "grad_norm": 6.15625,
      "learning_rate": 1.5940366972477068e-05,
      "loss": 1.7178,
      "step": 1240
    },
    {
      "epoch": 0.41284403669724773,
      "grad_norm": 6.5625,
      "learning_rate": 1.587483617300131e-05,
      "loss": 1.7003,
      "step": 1260
    },
    {
      "epoch": 0.41939711664482304,
      "grad_norm": 8.1875,
      "learning_rate": 1.580930537352556e-05,
      "loss": 1.6904,
      "step": 1280
    },
    {
      "epoch": 0.4259501965923984,
      "grad_norm": 6.75,
      "learning_rate": 1.5743774574049802e-05,
      "loss": 1.7392,
      "step": 1300
    },
    {
      "epoch": 0.4325032765399738,
      "grad_norm": 6.125,
      "learning_rate": 1.5678243774574052e-05,
      "loss": 1.7251,
      "step": 1320
    },
    {
      "epoch": 0.43905635648754915,
      "grad_norm": 6.15625,
      "learning_rate": 1.5612712975098298e-05,
      "loss": 1.7145,
      "step": 1340
    },
    {
      "epoch": 0.4456094364351245,
      "grad_norm": 6.75,
      "learning_rate": 1.5547182175622544e-05,
      "loss": 1.6966,
      "step": 1360
    },
    {
      "epoch": 0.4521625163826999,
      "grad_norm": 6.5625,
      "learning_rate": 1.548165137614679e-05,
      "loss": 1.7755,
      "step": 1380
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 7.09375,
      "learning_rate": 1.5416120576671036e-05,
      "loss": 1.74,
      "step": 1400
    },
    {
      "epoch": 0.46526867627785057,
      "grad_norm": 8.0,
      "learning_rate": 1.5350589777195282e-05,
      "loss": 1.7493,
      "step": 1420
    },
    {
      "epoch": 0.47182175622542594,
      "grad_norm": 6.78125,
      "learning_rate": 1.528505897771953e-05,
      "loss": 1.7739,
      "step": 1440
    },
    {
      "epoch": 0.4783748361730013,
      "grad_norm": 7.0625,
      "learning_rate": 1.5219528178243776e-05,
      "loss": 1.7191,
      "step": 1460
    },
    {
      "epoch": 0.4849279161205767,
      "grad_norm": 6.34375,
      "learning_rate": 1.5153997378768022e-05,
      "loss": 1.7188,
      "step": 1480
    },
    {
      "epoch": 0.49148099606815204,
      "grad_norm": 6.1875,
      "learning_rate": 1.5088466579292268e-05,
      "loss": 1.719,
      "step": 1500
    },
    {
      "epoch": 0.4980340760157274,
      "grad_norm": 6.4375,
      "learning_rate": 1.5022935779816514e-05,
      "loss": 1.6979,
      "step": 1520
    },
    {
      "epoch": 0.5045871559633027,
      "grad_norm": 6.75,
      "learning_rate": 1.4957404980340762e-05,
      "loss": 1.7558,
      "step": 1540
    },
    {
      "epoch": 0.5111402359108781,
      "grad_norm": 6.84375,
      "learning_rate": 1.4891874180865007e-05,
      "loss": 1.7429,
      "step": 1560
    },
    {
      "epoch": 0.5176933158584535,
      "grad_norm": 7.5,
      "learning_rate": 1.4826343381389254e-05,
      "loss": 1.694,
      "step": 1580
    },
    {
      "epoch": 0.5242463958060288,
      "grad_norm": 7.46875,
      "learning_rate": 1.4760812581913499e-05,
      "loss": 1.8062,
      "step": 1600
    },
    {
      "epoch": 0.5307994757536042,
      "grad_norm": 8.0625,
      "learning_rate": 1.4695281782437747e-05,
      "loss": 1.7815,
      "step": 1620
    },
    {
      "epoch": 0.5373525557011796,
      "grad_norm": 5.8125,
      "learning_rate": 1.4629750982961994e-05,
      "loss": 1.7507,
      "step": 1640
    },
    {
      "epoch": 0.5439056356487549,
      "grad_norm": 6.4375,
      "learning_rate": 1.4564220183486239e-05,
      "loss": 1.7363,
      "step": 1660
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 6.03125,
      "learning_rate": 1.4498689384010486e-05,
      "loss": 1.742,
      "step": 1680
    },
    {
      "epoch": 0.5570117955439057,
      "grad_norm": 7.6875,
      "learning_rate": 1.4433158584534733e-05,
      "loss": 1.721,
      "step": 1700
    },
    {
      "epoch": 0.563564875491481,
      "grad_norm": 6.84375,
      "learning_rate": 1.436762778505898e-05,
      "loss": 1.7382,
      "step": 1720
    },
    {
      "epoch": 0.5701179554390564,
      "grad_norm": 7.46875,
      "learning_rate": 1.4302096985583225e-05,
      "loss": 1.7559,
      "step": 1740
    },
    {
      "epoch": 0.5766710353866317,
      "grad_norm": 6.03125,
      "learning_rate": 1.4236566186107473e-05,
      "loss": 1.8061,
      "step": 1760
    },
    {
      "epoch": 0.583224115334207,
      "grad_norm": 6.8125,
      "learning_rate": 1.4171035386631717e-05,
      "loss": 1.7966,
      "step": 1780
    },
    {
      "epoch": 0.5897771952817824,
      "grad_norm": 7.1875,
      "learning_rate": 1.4105504587155965e-05,
      "loss": 1.6454,
      "step": 1800
    },
    {
      "epoch": 0.5963302752293578,
      "grad_norm": 7.40625,
      "learning_rate": 1.403997378768021e-05,
      "loss": 1.7052,
      "step": 1820
    },
    {
      "epoch": 0.6028833551769331,
      "grad_norm": 7.6875,
      "learning_rate": 1.3974442988204457e-05,
      "loss": 1.75,
      "step": 1840
    },
    {
      "epoch": 0.6094364351245085,
      "grad_norm": 7.4375,
      "learning_rate": 1.3908912188728703e-05,
      "loss": 1.6836,
      "step": 1860
    },
    {
      "epoch": 0.6159895150720839,
      "grad_norm": 7.5625,
      "learning_rate": 1.384338138925295e-05,
      "loss": 1.7142,
      "step": 1880
    },
    {
      "epoch": 0.6225425950196593,
      "grad_norm": 6.09375,
      "learning_rate": 1.3777850589777197e-05,
      "loss": 1.7283,
      "step": 1900
    },
    {
      "epoch": 0.6290956749672346,
      "grad_norm": 6.40625,
      "learning_rate": 1.3712319790301443e-05,
      "loss": 1.681,
      "step": 1920
    },
    {
      "epoch": 0.63564875491481,
      "grad_norm": 5.875,
      "learning_rate": 1.364678899082569e-05,
      "loss": 1.718,
      "step": 1940
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 7.5625,
      "learning_rate": 1.3581258191349935e-05,
      "loss": 1.7433,
      "step": 1960
    },
    {
      "epoch": 0.6487549148099607,
      "grad_norm": 7.46875,
      "learning_rate": 1.3515727391874183e-05,
      "loss": 1.7876,
      "step": 1980
    },
    {
      "epoch": 0.6553079947575361,
      "grad_norm": 6.6875,
      "learning_rate": 1.3450196592398427e-05,
      "loss": 1.6956,
      "step": 2000
    },
    {
      "epoch": 0.6618610747051114,
      "grad_norm": 6.28125,
      "learning_rate": 1.3384665792922675e-05,
      "loss": 1.7421,
      "step": 2020
    },
    {
      "epoch": 0.6684141546526867,
      "grad_norm": 7.09375,
      "learning_rate": 1.3319134993446921e-05,
      "loss": 1.7349,
      "step": 2040
    },
    {
      "epoch": 0.6749672346002621,
      "grad_norm": 6.4375,
      "learning_rate": 1.3253604193971167e-05,
      "loss": 1.7206,
      "step": 2060
    },
    {
      "epoch": 0.6815203145478375,
      "grad_norm": 7.5625,
      "learning_rate": 1.3188073394495413e-05,
      "loss": 1.7082,
      "step": 2080
    },
    {
      "epoch": 0.6880733944954128,
      "grad_norm": 7.1875,
      "learning_rate": 1.3122542595019661e-05,
      "loss": 1.7263,
      "step": 2100
    },
    {
      "epoch": 0.6946264744429882,
      "grad_norm": 8.3125,
      "learning_rate": 1.3057011795543905e-05,
      "loss": 1.7086,
      "step": 2120
    },
    {
      "epoch": 0.7011795543905636,
      "grad_norm": 7.15625,
      "learning_rate": 1.2991480996068153e-05,
      "loss": 1.8002,
      "step": 2140
    },
    {
      "epoch": 0.7077326343381389,
      "grad_norm": 7.0,
      "learning_rate": 1.2925950196592401e-05,
      "loss": 1.6714,
      "step": 2160
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 6.6875,
      "learning_rate": 1.2860419397116645e-05,
      "loss": 1.7548,
      "step": 2180
    },
    {
      "epoch": 0.7208387942332897,
      "grad_norm": 6.625,
      "learning_rate": 1.2794888597640893e-05,
      "loss": 1.6972,
      "step": 2200
    },
    {
      "epoch": 0.727391874180865,
      "grad_norm": 5.96875,
      "learning_rate": 1.2729357798165138e-05,
      "loss": 1.7833,
      "step": 2220
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 7.15625,
      "learning_rate": 1.2663826998689385e-05,
      "loss": 1.7637,
      "step": 2240
    },
    {
      "epoch": 0.7404980340760158,
      "grad_norm": 6.84375,
      "learning_rate": 1.2598296199213631e-05,
      "loss": 1.7275,
      "step": 2260
    },
    {
      "epoch": 0.747051114023591,
      "grad_norm": 7.125,
      "learning_rate": 1.253276539973788e-05,
      "loss": 1.735,
      "step": 2280
    },
    {
      "epoch": 0.7536041939711664,
      "grad_norm": 6.84375,
      "learning_rate": 1.2467234600262124e-05,
      "loss": 1.7005,
      "step": 2300
    },
    {
      "epoch": 0.7601572739187418,
      "grad_norm": 6.375,
      "learning_rate": 1.2401703800786371e-05,
      "loss": 1.7278,
      "step": 2320
    },
    {
      "epoch": 0.7667103538663171,
      "grad_norm": 7.1875,
      "learning_rate": 1.2336173001310616e-05,
      "loss": 1.6829,
      "step": 2340
    },
    {
      "epoch": 0.7732634338138925,
      "grad_norm": 6.21875,
      "learning_rate": 1.2270642201834864e-05,
      "loss": 1.6969,
      "step": 2360
    },
    {
      "epoch": 0.7798165137614679,
      "grad_norm": 8.125,
      "learning_rate": 1.220511140235911e-05,
      "loss": 1.7999,
      "step": 2380
    },
    {
      "epoch": 0.7863695937090432,
      "grad_norm": 5.9375,
      "learning_rate": 1.2139580602883356e-05,
      "loss": 1.6744,
      "step": 2400
    },
    {
      "epoch": 0.7929226736566186,
      "grad_norm": 6.625,
      "learning_rate": 1.2074049803407602e-05,
      "loss": 1.7172,
      "step": 2420
    },
    {
      "epoch": 0.799475753604194,
      "grad_norm": 7.0625,
      "learning_rate": 1.200851900393185e-05,
      "loss": 1.7073,
      "step": 2440
    },
    {
      "epoch": 0.8060288335517694,
      "grad_norm": 6.78125,
      "learning_rate": 1.1942988204456096e-05,
      "loss": 1.7102,
      "step": 2460
    },
    {
      "epoch": 0.8125819134993447,
      "grad_norm": 6.71875,
      "learning_rate": 1.1877457404980342e-05,
      "loss": 1.7577,
      "step": 2480
    },
    {
      "epoch": 0.8191349934469201,
      "grad_norm": 6.96875,
      "learning_rate": 1.181192660550459e-05,
      "loss": 1.7173,
      "step": 2500
    },
    {
      "epoch": 0.8256880733944955,
      "grad_norm": 6.21875,
      "learning_rate": 1.1746395806028834e-05,
      "loss": 1.7,
      "step": 2520
    },
    {
      "epoch": 0.8322411533420708,
      "grad_norm": 6.5,
      "learning_rate": 1.1680865006553082e-05,
      "loss": 1.7254,
      "step": 2540
    },
    {
      "epoch": 0.8387942332896461,
      "grad_norm": 7.71875,
      "learning_rate": 1.1615334207077326e-05,
      "loss": 1.6969,
      "step": 2560
    },
    {
      "epoch": 0.8453473132372215,
      "grad_norm": 6.65625,
      "learning_rate": 1.1549803407601574e-05,
      "loss": 1.7283,
      "step": 2580
    },
    {
      "epoch": 0.8519003931847968,
      "grad_norm": 6.75,
      "learning_rate": 1.148427260812582e-05,
      "loss": 1.743,
      "step": 2600
    },
    {
      "epoch": 0.8584534731323722,
      "grad_norm": 8.25,
      "learning_rate": 1.1418741808650066e-05,
      "loss": 1.7028,
      "step": 2620
    },
    {
      "epoch": 0.8650065530799476,
      "grad_norm": 5.9375,
      "learning_rate": 1.1353211009174312e-05,
      "loss": 1.8152,
      "step": 2640
    },
    {
      "epoch": 0.8715596330275229,
      "grad_norm": 6.5625,
      "learning_rate": 1.128768020969856e-05,
      "loss": 1.7463,
      "step": 2660
    },
    {
      "epoch": 0.8781127129750983,
      "grad_norm": 6.34375,
      "learning_rate": 1.1222149410222804e-05,
      "loss": 1.6764,
      "step": 2680
    },
    {
      "epoch": 0.8846657929226737,
      "grad_norm": 5.625,
      "learning_rate": 1.1156618610747052e-05,
      "loss": 1.6609,
      "step": 2700
    },
    {
      "epoch": 0.891218872870249,
      "grad_norm": 6.78125,
      "learning_rate": 1.10910878112713e-05,
      "loss": 1.6994,
      "step": 2720
    },
    {
      "epoch": 0.8977719528178244,
      "grad_norm": 7.25,
      "learning_rate": 1.1025557011795544e-05,
      "loss": 1.7419,
      "step": 2740
    },
    {
      "epoch": 0.9043250327653998,
      "grad_norm": 6.375,
      "learning_rate": 1.0960026212319792e-05,
      "loss": 1.7478,
      "step": 2760
    },
    {
      "epoch": 0.9108781127129751,
      "grad_norm": 5.84375,
      "learning_rate": 1.0894495412844036e-05,
      "loss": 1.7285,
      "step": 2780
    },
    {
      "epoch": 0.9174311926605505,
      "grad_norm": 6.5,
      "learning_rate": 1.0828964613368284e-05,
      "loss": 1.7158,
      "step": 2800
    },
    {
      "epoch": 0.9239842726081258,
      "grad_norm": 7.6875,
      "learning_rate": 1.076343381389253e-05,
      "loss": 1.7041,
      "step": 2820
    },
    {
      "epoch": 0.9305373525557011,
      "grad_norm": 6.09375,
      "learning_rate": 1.0697903014416776e-05,
      "loss": 1.6707,
      "step": 2840
    },
    {
      "epoch": 0.9370904325032765,
      "grad_norm": 5.84375,
      "learning_rate": 1.0632372214941023e-05,
      "loss": 1.6874,
      "step": 2860
    },
    {
      "epoch": 0.9436435124508519,
      "grad_norm": 6.34375,
      "learning_rate": 1.056684141546527e-05,
      "loss": 1.7168,
      "step": 2880
    },
    {
      "epoch": 0.9501965923984272,
      "grad_norm": 7.15625,
      "learning_rate": 1.0501310615989515e-05,
      "loss": 1.7575,
      "step": 2900
    },
    {
      "epoch": 0.9567496723460026,
      "grad_norm": 6.75,
      "learning_rate": 1.0435779816513762e-05,
      "loss": 1.777,
      "step": 2920
    },
    {
      "epoch": 0.963302752293578,
      "grad_norm": 5.96875,
      "learning_rate": 1.0370249017038007e-05,
      "loss": 1.7052,
      "step": 2940
    },
    {
      "epoch": 0.9698558322411533,
      "grad_norm": 6.15625,
      "learning_rate": 1.0304718217562255e-05,
      "loss": 1.6868,
      "step": 2960
    },
    {
      "epoch": 0.9764089121887287,
      "grad_norm": 5.65625,
      "learning_rate": 1.0239187418086502e-05,
      "loss": 1.7111,
      "step": 2980
    },
    {
      "epoch": 0.9829619921363041,
      "grad_norm": 6.75,
      "learning_rate": 1.0173656618610749e-05,
      "loss": 1.7429,
      "step": 3000
    },
    {
      "epoch": 0.9895150720838795,
      "grad_norm": 6.78125,
      "learning_rate": 1.0108125819134995e-05,
      "loss": 1.7614,
      "step": 3020
    },
    {
      "epoch": 0.9960681520314548,
      "grad_norm": 6.59375,
      "learning_rate": 1.004259501965924e-05,
      "loss": 1.7061,
      "step": 3040
    },
    {
      "epoch": 1.0026212319790302,
      "grad_norm": 7.1875,
      "learning_rate": 9.977064220183487e-06,
      "loss": 1.6589,
      "step": 3060
    },
    {
      "epoch": 1.0091743119266054,
      "grad_norm": 6.3125,
      "learning_rate": 9.911533420707733e-06,
      "loss": 1.6268,
      "step": 3080
    },
    {
      "epoch": 1.015727391874181,
      "grad_norm": 6.375,
      "learning_rate": 9.84600262123198e-06,
      "loss": 1.6624,
      "step": 3100
    },
    {
      "epoch": 1.0222804718217562,
      "grad_norm": 6.84375,
      "learning_rate": 9.780471821756227e-06,
      "loss": 1.6321,
      "step": 3120
    },
    {
      "epoch": 1.0288335517693317,
      "grad_norm": 7.25,
      "learning_rate": 9.714941022280473e-06,
      "loss": 1.6277,
      "step": 3140
    },
    {
      "epoch": 1.035386631716907,
      "grad_norm": 5.75,
      "learning_rate": 9.649410222804719e-06,
      "loss": 1.646,
      "step": 3160
    },
    {
      "epoch": 1.0419397116644824,
      "grad_norm": 7.46875,
      "learning_rate": 9.583879423328965e-06,
      "loss": 1.6568,
      "step": 3180
    },
    {
      "epoch": 1.0484927916120577,
      "grad_norm": 7.125,
      "learning_rate": 9.518348623853211e-06,
      "loss": 1.6827,
      "step": 3200
    },
    {
      "epoch": 1.0550458715596331,
      "grad_norm": 7.25,
      "learning_rate": 9.452817824377459e-06,
      "loss": 1.7026,
      "step": 3220
    },
    {
      "epoch": 1.0615989515072084,
      "grad_norm": 6.3125,
      "learning_rate": 9.387287024901705e-06,
      "loss": 1.6772,
      "step": 3240
    },
    {
      "epoch": 1.0681520314547837,
      "grad_norm": 7.84375,
      "learning_rate": 9.321756225425951e-06,
      "loss": 1.6329,
      "step": 3260
    },
    {
      "epoch": 1.0747051114023591,
      "grad_norm": 6.75,
      "learning_rate": 9.256225425950197e-06,
      "loss": 1.6618,
      "step": 3280
    },
    {
      "epoch": 1.0812581913499344,
      "grad_norm": 6.53125,
      "learning_rate": 9.190694626474443e-06,
      "loss": 1.6634,
      "step": 3300
    },
    {
      "epoch": 1.0878112712975099,
      "grad_norm": 6.6875,
      "learning_rate": 9.12516382699869e-06,
      "loss": 1.597,
      "step": 3320
    },
    {
      "epoch": 1.0943643512450851,
      "grad_norm": 6.84375,
      "learning_rate": 9.059633027522935e-06,
      "loss": 1.6361,
      "step": 3340
    },
    {
      "epoch": 1.1009174311926606,
      "grad_norm": 6.28125,
      "learning_rate": 8.994102228047183e-06,
      "loss": 1.6184,
      "step": 3360
    },
    {
      "epoch": 1.1074705111402359,
      "grad_norm": 7.3125,
      "learning_rate": 8.92857142857143e-06,
      "loss": 1.6808,
      "step": 3380
    },
    {
      "epoch": 1.1140235910878113,
      "grad_norm": 6.09375,
      "learning_rate": 8.863040629095675e-06,
      "loss": 1.652,
      "step": 3400
    },
    {
      "epoch": 1.1205766710353866,
      "grad_norm": 7.15625,
      "learning_rate": 8.797509829619923e-06,
      "loss": 1.6392,
      "step": 3420
    },
    {
      "epoch": 1.127129750982962,
      "grad_norm": 6.21875,
      "learning_rate": 8.73197903014417e-06,
      "loss": 1.6343,
      "step": 3440
    },
    {
      "epoch": 1.1336828309305373,
      "grad_norm": 7.21875,
      "learning_rate": 8.666448230668415e-06,
      "loss": 1.6307,
      "step": 3460
    },
    {
      "epoch": 1.1402359108781126,
      "grad_norm": 6.90625,
      "learning_rate": 8.600917431192661e-06,
      "loss": 1.6887,
      "step": 3480
    },
    {
      "epoch": 1.146788990825688,
      "grad_norm": 7.1875,
      "learning_rate": 8.535386631716907e-06,
      "loss": 1.705,
      "step": 3500
    },
    {
      "epoch": 1.1533420707732633,
      "grad_norm": 7.5,
      "learning_rate": 8.469855832241154e-06,
      "loss": 1.655,
      "step": 3520
    },
    {
      "epoch": 1.1598951507208388,
      "grad_norm": 7.9375,
      "learning_rate": 8.4043250327654e-06,
      "loss": 1.6563,
      "step": 3540
    },
    {
      "epoch": 1.166448230668414,
      "grad_norm": 7.53125,
      "learning_rate": 8.338794233289646e-06,
      "loss": 1.6538,
      "step": 3560
    },
    {
      "epoch": 1.1730013106159896,
      "grad_norm": 7.0625,
      "learning_rate": 8.273263433813893e-06,
      "loss": 1.6904,
      "step": 3580
    },
    {
      "epoch": 1.1795543905635648,
      "grad_norm": 9.625,
      "learning_rate": 8.20773263433814e-06,
      "loss": 1.6675,
      "step": 3600
    },
    {
      "epoch": 1.1861074705111403,
      "grad_norm": 6.71875,
      "learning_rate": 8.142201834862386e-06,
      "loss": 1.5922,
      "step": 3620
    },
    {
      "epoch": 1.1926605504587156,
      "grad_norm": 6.71875,
      "learning_rate": 8.076671035386633e-06,
      "loss": 1.6648,
      "step": 3640
    },
    {
      "epoch": 1.199213630406291,
      "grad_norm": 7.28125,
      "learning_rate": 8.01114023591088e-06,
      "loss": 1.7667,
      "step": 3660
    },
    {
      "epoch": 1.2057667103538663,
      "grad_norm": 6.3125,
      "learning_rate": 7.945609436435126e-06,
      "loss": 1.6416,
      "step": 3680
    },
    {
      "epoch": 1.2123197903014418,
      "grad_norm": 6.125,
      "learning_rate": 7.880078636959372e-06,
      "loss": 1.6403,
      "step": 3700
    },
    {
      "epoch": 1.218872870249017,
      "grad_norm": 6.375,
      "learning_rate": 7.814547837483618e-06,
      "loss": 1.6203,
      "step": 3720
    },
    {
      "epoch": 1.2254259501965925,
      "grad_norm": 7.75,
      "learning_rate": 7.749017038007864e-06,
      "loss": 1.6251,
      "step": 3740
    },
    {
      "epoch": 1.2319790301441678,
      "grad_norm": 7.84375,
      "learning_rate": 7.68348623853211e-06,
      "loss": 1.7346,
      "step": 3760
    },
    {
      "epoch": 1.238532110091743,
      "grad_norm": 6.84375,
      "learning_rate": 7.617955439056357e-06,
      "loss": 1.6584,
      "step": 3780
    },
    {
      "epoch": 1.2450851900393185,
      "grad_norm": 7.6875,
      "learning_rate": 7.552424639580603e-06,
      "loss": 1.6657,
      "step": 3800
    },
    {
      "epoch": 1.2516382699868938,
      "grad_norm": 7.65625,
      "learning_rate": 7.48689384010485e-06,
      "loss": 1.6561,
      "step": 3820
    },
    {
      "epoch": 1.2581913499344692,
      "grad_norm": 6.21875,
      "learning_rate": 7.421363040629096e-06,
      "loss": 1.6581,
      "step": 3840
    },
    {
      "epoch": 1.2647444298820445,
      "grad_norm": 6.625,
      "learning_rate": 7.355832241153342e-06,
      "loss": 1.6345,
      "step": 3860
    },
    {
      "epoch": 1.27129750982962,
      "grad_norm": 6.1875,
      "learning_rate": 7.290301441677588e-06,
      "loss": 1.5837,
      "step": 3880
    },
    {
      "epoch": 1.2778505897771952,
      "grad_norm": 7.28125,
      "learning_rate": 7.224770642201836e-06,
      "loss": 1.613,
      "step": 3900
    },
    {
      "epoch": 1.2844036697247707,
      "grad_norm": 7.25,
      "learning_rate": 7.159239842726082e-06,
      "loss": 1.6932,
      "step": 3920
    },
    {
      "epoch": 1.290956749672346,
      "grad_norm": 7.0625,
      "learning_rate": 7.093709043250329e-06,
      "loss": 1.6146,
      "step": 3940
    },
    {
      "epoch": 1.2975098296199215,
      "grad_norm": 6.46875,
      "learning_rate": 7.028178243774575e-06,
      "loss": 1.6511,
      "step": 3960
    },
    {
      "epoch": 1.3040629095674967,
      "grad_norm": 7.4375,
      "learning_rate": 6.962647444298821e-06,
      "loss": 1.6737,
      "step": 3980
    },
    {
      "epoch": 1.310615989515072,
      "grad_norm": 7.3125,
      "learning_rate": 6.897116644823067e-06,
      "loss": 1.6492,
      "step": 4000
    },
    {
      "epoch": 1.3171690694626474,
      "grad_norm": 5.9375,
      "learning_rate": 6.831585845347314e-06,
      "loss": 1.6929,
      "step": 4020
    },
    {
      "epoch": 1.323722149410223,
      "grad_norm": 7.8125,
      "learning_rate": 6.76605504587156e-06,
      "loss": 1.5961,
      "step": 4040
    },
    {
      "epoch": 1.3302752293577982,
      "grad_norm": 7.375,
      "learning_rate": 6.700524246395806e-06,
      "loss": 1.6537,
      "step": 4060
    },
    {
      "epoch": 1.3368283093053734,
      "grad_norm": 5.6875,
      "learning_rate": 6.6349934469200524e-06,
      "loss": 1.6656,
      "step": 4080
    },
    {
      "epoch": 1.343381389252949,
      "grad_norm": 6.65625,
      "learning_rate": 6.569462647444299e-06,
      "loss": 1.5962,
      "step": 4100
    },
    {
      "epoch": 1.3499344692005242,
      "grad_norm": 7.65625,
      "learning_rate": 6.5039318479685454e-06,
      "loss": 1.6586,
      "step": 4120
    },
    {
      "epoch": 1.3564875491480997,
      "grad_norm": 6.09375,
      "learning_rate": 6.4384010484927915e-06,
      "loss": 1.6658,
      "step": 4140
    },
    {
      "epoch": 1.363040629095675,
      "grad_norm": 6.1875,
      "learning_rate": 6.372870249017038e-06,
      "loss": 1.6738,
      "step": 4160
    },
    {
      "epoch": 1.3695937090432504,
      "grad_norm": 6.09375,
      "learning_rate": 6.307339449541285e-06,
      "loss": 1.5928,
      "step": 4180
    },
    {
      "epoch": 1.3761467889908257,
      "grad_norm": 5.875,
      "learning_rate": 6.2418086500655315e-06,
      "loss": 1.6542,
      "step": 4200
    },
    {
      "epoch": 1.382699868938401,
      "grad_norm": 9.8125,
      "learning_rate": 6.176277850589778e-06,
      "loss": 1.6834,
      "step": 4220
    },
    {
      "epoch": 1.3892529488859764,
      "grad_norm": 7.40625,
      "learning_rate": 6.1107470511140245e-06,
      "loss": 1.6342,
      "step": 4240
    },
    {
      "epoch": 1.3958060288335519,
      "grad_norm": 7.90625,
      "learning_rate": 6.045216251638271e-06,
      "loss": 1.6191,
      "step": 4260
    },
    {
      "epoch": 1.4023591087811271,
      "grad_norm": 7.6875,
      "learning_rate": 5.979685452162517e-06,
      "loss": 1.7358,
      "step": 4280
    },
    {
      "epoch": 1.4089121887287024,
      "grad_norm": 6.1875,
      "learning_rate": 5.914154652686764e-06,
      "loss": 1.7202,
      "step": 4300
    },
    {
      "epoch": 1.4154652686762779,
      "grad_norm": 6.65625,
      "learning_rate": 5.84862385321101e-06,
      "loss": 1.6367,
      "step": 4320
    },
    {
      "epoch": 1.4220183486238533,
      "grad_norm": 6.25,
      "learning_rate": 5.783093053735256e-06,
      "loss": 1.641,
      "step": 4340
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 6.65625,
      "learning_rate": 5.717562254259502e-06,
      "loss": 1.6095,
      "step": 4360
    },
    {
      "epoch": 1.4351245085190039,
      "grad_norm": 6.96875,
      "learning_rate": 5.652031454783749e-06,
      "loss": 1.6552,
      "step": 4380
    },
    {
      "epoch": 1.4416775884665793,
      "grad_norm": 6.03125,
      "learning_rate": 5.586500655307995e-06,
      "loss": 1.6841,
      "step": 4400
    },
    {
      "epoch": 1.4482306684141546,
      "grad_norm": 6.28125,
      "learning_rate": 5.520969855832241e-06,
      "loss": 1.5968,
      "step": 4420
    },
    {
      "epoch": 1.45478374836173,
      "grad_norm": 6.8125,
      "learning_rate": 5.455439056356489e-06,
      "loss": 1.6643,
      "step": 4440
    },
    {
      "epoch": 1.4613368283093053,
      "grad_norm": 6.40625,
      "learning_rate": 5.389908256880735e-06,
      "loss": 1.6507,
      "step": 4460
    },
    {
      "epoch": 1.4678899082568808,
      "grad_norm": 7.15625,
      "learning_rate": 5.324377457404981e-06,
      "loss": 1.6455,
      "step": 4480
    },
    {
      "epoch": 1.474442988204456,
      "grad_norm": 6.84375,
      "learning_rate": 5.258846657929228e-06,
      "loss": 1.6975,
      "step": 4500
    },
    {
      "epoch": 1.4809960681520313,
      "grad_norm": 7.90625,
      "learning_rate": 5.193315858453474e-06,
      "loss": 1.6604,
      "step": 4520
    },
    {
      "epoch": 1.4875491480996068,
      "grad_norm": 6.53125,
      "learning_rate": 5.12778505897772e-06,
      "loss": 1.6171,
      "step": 4540
    },
    {
      "epoch": 1.4941022280471823,
      "grad_norm": 7.25,
      "learning_rate": 5.062254259501966e-06,
      "loss": 1.6684,
      "step": 4560
    },
    {
      "epoch": 1.5006553079947575,
      "grad_norm": 7.15625,
      "learning_rate": 4.996723460026213e-06,
      "loss": 1.6579,
      "step": 4580
    },
    {
      "epoch": 1.5072083879423328,
      "grad_norm": 7.0,
      "learning_rate": 4.931192660550459e-06,
      "loss": 1.696,
      "step": 4600
    },
    {
      "epoch": 1.5137614678899083,
      "grad_norm": 6.625,
      "learning_rate": 4.865661861074705e-06,
      "loss": 1.5941,
      "step": 4620
    },
    {
      "epoch": 1.5203145478374838,
      "grad_norm": 6.125,
      "learning_rate": 4.800131061598952e-06,
      "loss": 1.6828,
      "step": 4640
    },
    {
      "epoch": 1.526867627785059,
      "grad_norm": 6.59375,
      "learning_rate": 4.734600262123198e-06,
      "loss": 1.6435,
      "step": 4660
    },
    {
      "epoch": 1.5334207077326343,
      "grad_norm": 7.5625,
      "learning_rate": 4.669069462647445e-06,
      "loss": 1.6014,
      "step": 4680
    },
    {
      "epoch": 1.5399737876802098,
      "grad_norm": 6.96875,
      "learning_rate": 4.603538663171691e-06,
      "loss": 1.6318,
      "step": 4700
    },
    {
      "epoch": 1.546526867627785,
      "grad_norm": 6.71875,
      "learning_rate": 4.538007863695937e-06,
      "loss": 1.6044,
      "step": 4720
    },
    {
      "epoch": 1.5530799475753603,
      "grad_norm": 8.4375,
      "learning_rate": 4.4724770642201834e-06,
      "loss": 1.6907,
      "step": 4740
    },
    {
      "epoch": 1.5596330275229358,
      "grad_norm": 5.96875,
      "learning_rate": 4.40694626474443e-06,
      "loss": 1.6054,
      "step": 4760
    },
    {
      "epoch": 1.5661861074705112,
      "grad_norm": 6.875,
      "learning_rate": 4.3414154652686765e-06,
      "loss": 1.6579,
      "step": 4780
    },
    {
      "epoch": 1.5727391874180865,
      "grad_norm": 5.59375,
      "learning_rate": 4.275884665792923e-06,
      "loss": 1.6136,
      "step": 4800
    },
    {
      "epoch": 1.5792922673656618,
      "grad_norm": 7.125,
      "learning_rate": 4.2103538663171695e-06,
      "loss": 1.6949,
      "step": 4820
    },
    {
      "epoch": 1.5858453473132372,
      "grad_norm": 6.375,
      "learning_rate": 4.1448230668414156e-06,
      "loss": 1.6588,
      "step": 4840
    },
    {
      "epoch": 1.5923984272608127,
      "grad_norm": 5.71875,
      "learning_rate": 4.0792922673656625e-06,
      "loss": 1.6434,
      "step": 4860
    },
    {
      "epoch": 1.598951507208388,
      "grad_norm": 8.5,
      "learning_rate": 4.013761467889909e-06,
      "loss": 1.6426,
      "step": 4880
    },
    {
      "epoch": 1.6055045871559632,
      "grad_norm": 8.0625,
      "learning_rate": 3.948230668414155e-06,
      "loss": 1.6504,
      "step": 4900
    },
    {
      "epoch": 1.6120576671035387,
      "grad_norm": 6.9375,
      "learning_rate": 3.882699868938402e-06,
      "loss": 1.5816,
      "step": 4920
    },
    {
      "epoch": 1.6186107470511142,
      "grad_norm": 6.53125,
      "learning_rate": 3.817169069462648e-06,
      "loss": 1.6713,
      "step": 4940
    },
    {
      "epoch": 1.6251638269986892,
      "grad_norm": 7.15625,
      "learning_rate": 3.751638269986894e-06,
      "loss": 1.6224,
      "step": 4960
    },
    {
      "epoch": 1.6317169069462647,
      "grad_norm": 7.0625,
      "learning_rate": 3.6861074705111403e-06,
      "loss": 1.6648,
      "step": 4980
    },
    {
      "epoch": 1.6382699868938402,
      "grad_norm": 6.8125,
      "learning_rate": 3.620576671035387e-06,
      "loss": 1.6435,
      "step": 5000
    },
    {
      "epoch": 1.6448230668414154,
      "grad_norm": 7.28125,
      "learning_rate": 3.5550458715596333e-06,
      "loss": 1.6482,
      "step": 5020
    },
    {
      "epoch": 1.6513761467889907,
      "grad_norm": 7.03125,
      "learning_rate": 3.48951507208388e-06,
      "loss": 1.7388,
      "step": 5040
    },
    {
      "epoch": 1.6579292267365662,
      "grad_norm": 7.125,
      "learning_rate": 3.4239842726081263e-06,
      "loss": 1.6758,
      "step": 5060
    },
    {
      "epoch": 1.6644823066841417,
      "grad_norm": 8.75,
      "learning_rate": 3.3584534731323724e-06,
      "loss": 1.7066,
      "step": 5080
    },
    {
      "epoch": 1.671035386631717,
      "grad_norm": 6.84375,
      "learning_rate": 3.292922673656619e-06,
      "loss": 1.6627,
      "step": 5100
    },
    {
      "epoch": 1.6775884665792922,
      "grad_norm": 7.59375,
      "learning_rate": 3.227391874180865e-06,
      "loss": 1.6922,
      "step": 5120
    },
    {
      "epoch": 1.6841415465268676,
      "grad_norm": 5.90625,
      "learning_rate": 3.1618610747051115e-06,
      "loss": 1.6983,
      "step": 5140
    },
    {
      "epoch": 1.6906946264744431,
      "grad_norm": 6.9375,
      "learning_rate": 3.0963302752293576e-06,
      "loss": 1.6328,
      "step": 5160
    },
    {
      "epoch": 1.6972477064220184,
      "grad_norm": 6.46875,
      "learning_rate": 3.0307994757536045e-06,
      "loss": 1.6016,
      "step": 5180
    },
    {
      "epoch": 1.7038007863695936,
      "grad_norm": 7.34375,
      "learning_rate": 2.965268676277851e-06,
      "loss": 1.6555,
      "step": 5200
    },
    {
      "epoch": 1.7103538663171691,
      "grad_norm": 6.125,
      "learning_rate": 2.899737876802097e-06,
      "loss": 1.7281,
      "step": 5220
    },
    {
      "epoch": 1.7169069462647444,
      "grad_norm": 5.8125,
      "learning_rate": 2.8342070773263436e-06,
      "loss": 1.6629,
      "step": 5240
    },
    {
      "epoch": 1.7234600262123196,
      "grad_norm": 6.65625,
      "learning_rate": 2.7686762778505897e-06,
      "loss": 1.6274,
      "step": 5260
    },
    {
      "epoch": 1.7300131061598951,
      "grad_norm": 6.65625,
      "learning_rate": 2.7031454783748362e-06,
      "loss": 1.6313,
      "step": 5280
    },
    {
      "epoch": 1.7365661861074706,
      "grad_norm": 6.9375,
      "learning_rate": 2.6376146788990823e-06,
      "loss": 1.6844,
      "step": 5300
    },
    {
      "epoch": 1.7431192660550459,
      "grad_norm": 7.1875,
      "learning_rate": 2.5720838794233293e-06,
      "loss": 1.6132,
      "step": 5320
    },
    {
      "epoch": 1.7496723460026211,
      "grad_norm": 6.5625,
      "learning_rate": 2.5065530799475758e-06,
      "loss": 1.6479,
      "step": 5340
    },
    {
      "epoch": 1.7562254259501966,
      "grad_norm": 7.15625,
      "learning_rate": 2.441022280471822e-06,
      "loss": 1.651,
      "step": 5360
    },
    {
      "epoch": 1.762778505897772,
      "grad_norm": 7.84375,
      "learning_rate": 2.3754914809960684e-06,
      "loss": 1.6472,
      "step": 5380
    },
    {
      "epoch": 1.7693315858453473,
      "grad_norm": 7.375,
      "learning_rate": 2.3099606815203144e-06,
      "loss": 1.6406,
      "step": 5400
    },
    {
      "epoch": 1.7758846657929226,
      "grad_norm": 7.5625,
      "learning_rate": 2.2444298820445614e-06,
      "loss": 1.664,
      "step": 5420
    },
    {
      "epoch": 1.782437745740498,
      "grad_norm": 7.84375,
      "learning_rate": 2.1788990825688075e-06,
      "loss": 1.5959,
      "step": 5440
    },
    {
      "epoch": 1.7889908256880735,
      "grad_norm": 7.53125,
      "learning_rate": 2.113368283093054e-06,
      "loss": 1.661,
      "step": 5460
    },
    {
      "epoch": 1.7955439056356488,
      "grad_norm": 6.75,
      "learning_rate": 2.0478374836173005e-06,
      "loss": 1.6587,
      "step": 5480
    },
    {
      "epoch": 1.802096985583224,
      "grad_norm": 7.3125,
      "learning_rate": 1.9823066841415466e-06,
      "loss": 1.6768,
      "step": 5500
    },
    {
      "epoch": 1.8086500655307995,
      "grad_norm": 6.5625,
      "learning_rate": 1.916775884665793e-06,
      "loss": 1.6339,
      "step": 5520
    },
    {
      "epoch": 1.8152031454783748,
      "grad_norm": 7.75,
      "learning_rate": 1.8512450851900396e-06,
      "loss": 1.6952,
      "step": 5540
    },
    {
      "epoch": 1.82175622542595,
      "grad_norm": 6.28125,
      "learning_rate": 1.7857142857142859e-06,
      "loss": 1.6747,
      "step": 5560
    },
    {
      "epoch": 1.8283093053735255,
      "grad_norm": 7.375,
      "learning_rate": 1.7201834862385322e-06,
      "loss": 1.6602,
      "step": 5580
    },
    {
      "epoch": 1.834862385321101,
      "grad_norm": 8.0,
      "learning_rate": 1.6546526867627785e-06,
      "loss": 1.6514,
      "step": 5600
    },
    {
      "epoch": 1.8414154652686763,
      "grad_norm": 6.1875,
      "learning_rate": 1.589121887287025e-06,
      "loss": 1.6047,
      "step": 5620
    },
    {
      "epoch": 1.8479685452162515,
      "grad_norm": 7.78125,
      "learning_rate": 1.5235910878112715e-06,
      "loss": 1.6022,
      "step": 5640
    },
    {
      "epoch": 1.854521625163827,
      "grad_norm": 5.59375,
      "learning_rate": 1.4580602883355178e-06,
      "loss": 1.6738,
      "step": 5660
    },
    {
      "epoch": 1.8610747051114025,
      "grad_norm": 6.1875,
      "learning_rate": 1.3925294888597643e-06,
      "loss": 1.7288,
      "step": 5680
    },
    {
      "epoch": 1.8676277850589778,
      "grad_norm": 7.09375,
      "learning_rate": 1.3269986893840106e-06,
      "loss": 1.7177,
      "step": 5700
    },
    {
      "epoch": 1.874180865006553,
      "grad_norm": 8.1875,
      "learning_rate": 1.261467889908257e-06,
      "loss": 1.6572,
      "step": 5720
    },
    {
      "epoch": 1.8807339449541285,
      "grad_norm": 6.5625,
      "learning_rate": 1.1959370904325034e-06,
      "loss": 1.6231,
      "step": 5740
    },
    {
      "epoch": 1.8872870249017037,
      "grad_norm": 6.3125,
      "learning_rate": 1.1304062909567497e-06,
      "loss": 1.6612,
      "step": 5760
    },
    {
      "epoch": 1.893840104849279,
      "grad_norm": 6.90625,
      "learning_rate": 1.064875491480996e-06,
      "loss": 1.7189,
      "step": 5780
    },
    {
      "epoch": 1.9003931847968545,
      "grad_norm": 8.3125,
      "learning_rate": 9.993446920052425e-07,
      "loss": 1.6556,
      "step": 5800
    },
    {
      "epoch": 1.90694626474443,
      "grad_norm": 7.28125,
      "learning_rate": 9.338138925294888e-07,
      "loss": 1.6164,
      "step": 5820
    },
    {
      "epoch": 1.9134993446920052,
      "grad_norm": 6.9375,
      "learning_rate": 8.682830930537353e-07,
      "loss": 1.579,
      "step": 5840
    },
    {
      "epoch": 1.9200524246395805,
      "grad_norm": 6.34375,
      "learning_rate": 8.027522935779817e-07,
      "loss": 1.6499,
      "step": 5860
    },
    {
      "epoch": 1.926605504587156,
      "grad_norm": 6.53125,
      "learning_rate": 7.37221494102228e-07,
      "loss": 1.626,
      "step": 5880
    },
    {
      "epoch": 1.9331585845347314,
      "grad_norm": 6.59375,
      "learning_rate": 6.716906946264745e-07,
      "loss": 1.7079,
      "step": 5900
    },
    {
      "epoch": 1.9397116644823067,
      "grad_norm": 7.21875,
      "learning_rate": 6.061598951507208e-07,
      "loss": 1.6722,
      "step": 5920
    },
    {
      "epoch": 1.946264744429882,
      "grad_norm": 7.34375,
      "learning_rate": 5.406290956749672e-07,
      "loss": 1.6478,
      "step": 5940
    },
    {
      "epoch": 1.9528178243774574,
      "grad_norm": 7.46875,
      "learning_rate": 4.7509829619921365e-07,
      "loss": 1.6232,
      "step": 5960
    },
    {
      "epoch": 1.959370904325033,
      "grad_norm": 6.09375,
      "learning_rate": 4.095674967234601e-07,
      "loss": 1.6795,
      "step": 5980
    },
    {
      "epoch": 1.9659239842726082,
      "grad_norm": 6.90625,
      "learning_rate": 3.4403669724770646e-07,
      "loss": 1.6577,
      "step": 6000
    },
    {
      "epoch": 1.9724770642201834,
      "grad_norm": 7.15625,
      "learning_rate": 2.7850589777195286e-07,
      "loss": 1.6482,
      "step": 6020
    },
    {
      "epoch": 1.979030144167759,
      "grad_norm": 6.875,
      "learning_rate": 2.1297509829619921e-07,
      "loss": 1.6786,
      "step": 6040
    },
    {
      "epoch": 1.9855832241153342,
      "grad_norm": 6.75,
      "learning_rate": 1.4744429882044562e-07,
      "loss": 1.6389,
      "step": 6060
    },
    {
      "epoch": 1.9921363040629094,
      "grad_norm": 7.09375,
      "learning_rate": 8.191349934469201e-08,
      "loss": 1.6904,
      "step": 6080
    },
    {
      "epoch": 1.998689384010485,
      "grad_norm": 6.9375,
      "learning_rate": 1.6382699868938402e-08,
      "loss": 1.6247,
      "step": 6100
    }
  ],
  "logging_steps": 20,
  "max_steps": 6104,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.583342880784384e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
