{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9053f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daijunjie/miniconda3/envs/langchain-env/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/Users/daijunjie/miniconda3/envs/langchain-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model_path = os.path.expanduser('~/models/Qwen/Qwen3-0.6B')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=device,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce9051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2TokenizerFast(name_or_path='/Users/daijunjie/models/Qwen/Qwen3-0.6B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c604ff",
   "metadata": {},
   "source": [
    "# 1.Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03c3d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: {'input_ids': tensor([[ 56568, 101909,  30709, 104949,      0]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='mps:0')}\n",
      "decoded: 你是一个小模型!\n"
     ]
    }
   ],
   "source": [
    "from torch import return_types\n",
    "\n",
    "\n",
    "text = '你是一个小模型!'\n",
    "# encoded = tokenizer.encode(text)\n",
    "encoded = tokenizer([text],return_tensors='pt').to('mps')\n",
    "print(f'encoded: {encoded}')\n",
    "\n",
    "print(f'decoded: {tokenizer.decode(encoded['input_ids'][0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44009f43",
   "metadata": {},
   "source": [
    "## 把tokenzier 编码结果作为模型的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a8c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daijunjie/miniconda3/envs/langchain-env/lib/python3.12/site-packages/transformers/pytorch_utils.py:335: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56568, 101909,  30709, 104949,      0,  49434,    239,  99601,  30534,\n",
       "          61443,  46944, 101888, 104455,  18493,  99328, 101047,  99892,  33108,\n",
       "          99564,   9370, 104734,   3837,  85106,  61443,  46944, 104734, 106367,\n",
       "           3837, 106367,  30534, 102298,   2073, 104455,    854,  33108,   2073,\n",
       "          99328,    854, 100369, 105291,   1773, 106367,  30534, 101137, 104380,\n",
       "         104734, 104272,   3837, 106367,  30534, 110485,   5373,  18830, 104380,\n",
       "          33071,   5373,  18830,  99556, 100162,   1773, 106367,  85106, 102298,\n",
       "           2073,  99328,    854,  33108,   2073, 104455,    854, 100369, 105291,\n",
       "           3837,  91572, 106367,  30534, 101137, 104380, 104734, 104272,   3837,\n",
       "         106367,  30534, 110485,   5373,  18830, 104380,  33071,   5373,  18830,\n",
       "          99556, 100162,   1773, 106367,  85106, 102298,   2073,  99328,    854,\n",
       "          33108,   2073, 104455,    854, 100369, 105291,   3837,  91572, 106367,\n",
       "          30534, 101137, 104380, 104734, 104272,   3837, 106367,  30534, 110485,\n",
       "           5373,  18830, 104380,  33071,   5373,  18830,  99556, 100162,   1773,\n",
       "         106367,  85106, 102298,   2073,  99328,    854,  33108,   2073, 104455,\n",
       "            854, 100369, 105291,   3837,  91572, 106367,  30534, 101137, 104380,\n",
       "         104734, 104272,   3837, 106367,  30534, 110485,   5373,  18830, 104380,\n",
       "          33071,   5373,  18830,  99556, 100162,   1773, 106367,  85106, 102298,\n",
       "           2073,  99328,    854,  33108,   2073, 104455,    854, 100369, 105291,\n",
       "           3837,  91572, 106367,  30534, 101137, 104380, 104734, 104272,   3837,\n",
       "         106367,  30534, 110485,   5373,  18830, 104380,  33071,   5373,  18830,\n",
       "          99556, 100162,   8997,  99692,   3837,  99601,  35946,  85106,  99663,\n",
       "          20002,  61443]], device='mps:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**encoded,max_length = 200)\n",
    "# generated_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f18399",
   "metadata": {},
   "source": [
    "# 用tokenzier解码模型的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97052e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你是一个小模型! 我现在要写一个关于人工智能在社会中的应用和影响的论文，需要写一个论文题目，题目要包含“人工智能”和“社会”两个关键词。题目要符合学术论文的要求，题目要简洁、有学术性、有研究价值。题目需要包含“社会”和“人工智能”两个关键词，同时题目要符合学术论文的要求，题目要简洁、有学术性、有研究价值。题目需要包含“社会”和“人工智能”两个关键词，同时题目要符合学术论文的要求，题目要简洁、有学术性、有研究价值。题目需要包含“社会”和“人工智能”两个关键词，同时题目要符合学术论文的要求，题目要简洁、有学术性、有研究价值。题目需要包含“社会”和“人工智能”两个关键词，同时题目要符合学术论文的要求，题目要简洁、有学术性、有研究价值。\\n好的，现在我需要帮用户写'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode(generated_ids[0])\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbae853",
   "metadata": {},
   "source": [
    "# 2.模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8cbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n黄仙洞是什么?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text = \"黄仙洞是什么?\"\n",
    "messages = [{'role': 'user', 'content': user_text}]\n",
    "user_formatted_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "user_formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23572da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198,  99789, 100717, 100743, 102021,     30, 151645,\n",
       "            198, 151644,  77091,    198]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer(user_formatted_text,return_tensors='pt').to('mps')\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1737461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daijunjie/miniconda3/envs/langchain-env/lib/python3.12/site-packages/transformers/pytorch_utils.py:335: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    }
   ],
   "source": [
    "# 3. 生成回答\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# 4. 解码回答（只取新生成的部分）\n",
    "generated_tokens = generated_ids[0][len(model_inputs['input_ids'][0]):]\n",
    "response = tokenizer.decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2a6635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "好的，用户问“黄仙洞是什么？”。首先，我需要确认用户指的是哪个黄仙洞。因为可能存在多个同名的地点，比如中国的一个地方，或者国外的景点。首先，我应该先明确用户所说的“黄仙洞”具体指的是哪个地方。如果用户没有提供更多信息，可能需要进一步询问。\n",
      "\n",
      "接下来，我需要考虑用户的潜在需求。用户可能对黄仙洞感兴趣，想知道它的位置、特色或者有什么特别之处。可能用户在旅行或者计划游览时遇到了这个名称，或者有其他目的。因此，我需要提供准确的信息，同时保持回答的友好和帮助性。\n",
      "\n",
      "另外，用户可能没有意识到自己可能指的是多个地方，所以需要明确回答，避免用户混淆。同时，考虑到可能的文化或地理差异，需要确保信息的准确性和相关性。\n",
      "\n",
      "最后，检查回答是否符合中文表达习惯，信息是否完整，是否需要进一步帮助。确保回答简洁明了，同时涵盖主要方面，比如地理位置、特色、历史或文化元素，以满足用户的需求。\n",
      "</think>\n",
      "\n",
      "黄仙洞是位于中国湖南省张家界市的一个著名自然景区。它以独特的喀斯特地貌闻名，是世界自然遗产地之一。洞内有“天洞”、“仙洞”等名称，洞内有洞穴、瀑布、溶洞等多种自然景观，环境幽美，被誉为“中国喀斯特地貌的典范”。黄仙洞不仅是中国著名的旅游景点，也是研究喀斯特地质学的重要地点。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a5127",
   "metadata": {},
   "source": [
    "## 模型推理封装为函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e72418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daijunjie/miniconda3/envs/langchain-env/lib/python3.12/site-packages/transformers/pytorch_utils.py:335: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: 介绍下湖北荆门黄仙洞\n",
      "回答: <think>\n",
      "好的，用户想了解湖北荆门黄仙洞。首先，我需要确认这个洞穴的位置和基本信息。荆门位于湖北省，黄仙洞应该是一个特定的自然景观。\n",
      "\n",
      "接下来，我需要考虑用户的需求。他们可能对地质、历史、文化或者旅游感兴趣。用户可能想知道洞穴的形成过程，或者有什么特别的活动。需要确保信息准确，避免错误。\n",
      "\n",
      "然后，我要检查是否有相关的资料或可靠的信息来源。可能需要查阅地质资料或旅游指南，确保信息的准确性。同时，要使用中文进行介绍，保持回答的流畅和自然。\n",
      "\n",
      "另外，要注意用户可能的深层需求，比如是否对自然景观感兴趣，或者有旅游计划。回答时要突出洞穴的独特之处，比如地质特征或文化意义，以吸引他们进一步探索。\n",
      "\n",
      "最后，确保回答结构清晰，分点列出关键信息，方便用户快速获取。同时，保持口语化，避免过于正式，让用户感觉亲切自然。\n",
      "</think>\n",
      "\n",
      "湖北荆门黄仙洞位于荆门市，是湖北省著名的地质和自然景观之一。以下是关于黄仙洞的基本介绍：\n",
      "\n",
      "1. **地理位置**：黄仙洞位于荆门市东北部，地处长江中游的黄龙山东麓，是长江水系的重要组成部分。\n",
      "\n",
      "2. **地质特征**：洞穴形成于侏罗纪时期，由石灰岩层构成，具有独特的地质结构和水系景观，是典型的溶洞地貌。\n",
      "\n",
      "3. **历史意义**：黄仙洞曾是古代祭祀活动的场所，是荆门地区重要的文化遗迹，现为荆门市重点文物保护单位。\n",
      "\n",
      "4. **旅游特色**：洞内设有观景台和生态观景台，游客可俯瞰洞穴全景，感受自然的壮丽景色。洞穴内部还保存有丰富的化石和植物标本。\n",
      "\n",
      "5. **文化内涵**：黄仙洞不仅具有自然景观，还承载着荆门的历史文化，是研究地质和人类文明的重要资源。\n",
      "\n",
      "黄仙洞以其独特的地质构造和丰富的自然景观，吸引了众多游客前来探秘，是湖北省重要的自然与文化旅游目的地之一。\n"
     ]
    }
   ],
   "source": [
    "def chat_with_model(user_input, model, tokenizer):\n",
    "    \"\"\"完整的聊天推理函数\"\"\"\n",
    "    # 1. 格式化输入\n",
    "    messages = [{'role': 'user', 'content': user_input}]\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 2. 分词并转换为tensor\n",
    "    model_inputs = tokenizer(formatted_text, return_tensors='pt').to('mps')\n",
    "    \n",
    "    # 3. 生成回答\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 4. 解码回答（只取新生成的部分）\n",
    "    generated_tokens = generated_ids[0][len(model_inputs['input_ids'][0]):]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 使用示例\n",
    "user_question = \"介绍下湖北荆门黄仙洞\"\n",
    "answer = chat_with_model(user_question, model, tokenizer)\n",
    "print(f\"问题: {user_question}\")\n",
    "print(f\"回答: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
