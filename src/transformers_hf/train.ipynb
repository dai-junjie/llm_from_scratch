{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2139d22",
   "metadata": {},
   "source": [
    "# train\n",
    "huggingface提供的训练是经过抽象的，分为trainingArguments，training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab72618",
   "metadata": {},
   "source": [
    "## 1 .准备模型&分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67217378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model_path = os.path.expanduser('~/models/Qwen/Qwen3-0.6B')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=device,torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer('你好')) # 只有input_ids和attention_mask\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724df5e",
   "metadata": {},
   "source": [
    "## 3.准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "data_dir = os.path.expanduser(\"~/datasets/alpaca-gpt4-data-zh\")\n",
    "ds = load_dataset(\"json\",data_dir=data_dir,split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb346e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "数据集的每一个item有instruction，input和output三个\n",
    "\n",
    "\"\"\"\n",
    "ds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    f\"\"\"\n",
    "    处理数据集，用来把数据改造成适合训练的格式.\n",
    "    对话数据集，这里的每个example都是一次对话，只有问题&回答\n",
    "    这里需要加上Human和Assistant标签\n",
    "    Args:\n",
    "        example字典结构:\n",
    "            'instruction':字符串\n",
    "            'input':str\n",
    "            'output':str\n",
    "        \n",
    "    \"\"\"\n",
    "    MAX_LENGTH=256\n",
    "    # 训练用的instruction由instruction和input构成\n",
    "    instruction = tokenizer(\n",
    "        \"\\n\".join([\"Human: \"+example[\"instruction\"], example[\"input\"]]).strip()\n",
    "        +\"\\n\\nAssistant: \")\n",
    "    # 响应就是output\n",
    "    response = tokenizer(example['output'] + tokenizer.eos_token)\n",
    "    # 模型训练的内容就是instruction+output这一次对话，\n",
    "    # 以便后面给出instruction模型能够自回归生成后面的output\n",
    "    input_ids = instruction['input_ids'] + response['input_ids']\n",
    "    atten_mask = instruction['attention_mask']+response['attention_mask']\n",
    "    # Create labels with -100 for instruction part (ignored in loss) and actual tokens for response\n",
    "    labels = [-100] * len(instruction['input_ids']) + response['input_ids']\n",
    "    \n",
    "    # 最大长度截断\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        atten_mask = atten_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\":input_ids,\n",
    "        \"attention_mask\":atten_mask,\n",
    "        \"labels\":labels\n",
    "    }\n",
    "\n",
    "example = process_func(ds[0])\n",
    "print(f'训练用的example:',example)\n",
    "print(f'解码后:\\n{tokenizer.decode(example[\"input_ids\"])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化数据,删除原始列名，默认是保留的(instruction,input,output)\n",
    "train_ds = ds.map(process_func,remove_columns = ds.column_names)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f6865",
   "metadata": {},
   "source": [
    "## 准备训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea350ae",
   "metadata": {},
   "source": [
    "### 训练参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model/qwen3-0.6B\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    # per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=1e-4,\n",
    "    # eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    logging_strategy='steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer,padding=True),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385da27",
   "metadata": {},
   "source": [
    "# 使用chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e0523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daijunjie/miniconda3/envs/langchain-env/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/Users/daijunjie/miniconda3/envs/langchain-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 48818/48818 [00:29<00:00, 1671.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model_path = os.path.expanduser('~/models/Qwen/Qwen3-0.6B')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float32,  # 使用BF16\n",
    "    # attn_implementation=\"flash_attention_2\",  # 如果支持Flash Attention\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "data_dir = os.path.expanduser(\"~/datasets/alpaca-gpt4-data-zh\")\n",
    "ds = load_dataset(\"json\",data_dir=data_dir,split=\"train\")\n",
    "\n",
    "\n",
    "def process_func(example):\n",
    "    f\"\"\"\n",
    "    处理数据集，用来把数据改造成适合训练的格式.\n",
    "    对话数据集，这里的每个example都是一次对话，只有问题&回答\n",
    "    这里需要加上Human和Assistant标签\n",
    "    Args:\n",
    "        example字典结构:\n",
    "            'instruction':字符串\n",
    "            'input':str\n",
    "            'output':str\n",
    "        使用tokenizer的apply_chat_template方法\n",
    "    \"\"\"\n",
    "    MAX_LENGTH=256\n",
    "    user_content = example['instruction']\n",
    "    if example['input'].strip():\n",
    "        user_content += \"\\n\" + example['input']\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\":\"user\",\"content\":user_content},\n",
    "        {\"role\":\"assistant\",\"content\":example['output']}\n",
    "    ]\n",
    "    # 使用chat template\n",
    "    full_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # 分别处理用户和助手部分\n",
    "    user_messages  = [{\"role\":\"user\",\"content\":user_content}]\n",
    "    user_text = tokenizer.apply_chat_template(\n",
    "        user_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True # 添加assistant开始标记\n",
    "    )\n",
    "    # tokenize\n",
    "    full_tokens = tokenizer(full_text,add_special_tokens=False)\n",
    "    user_tokens = tokenizer(user_text,add_special_tokens=False)\n",
    "    \n",
    "    input_ids = full_tokens['input_ids']\n",
    "    attention_mask = full_tokens['attention_mask']\n",
    "    # 创建labels 用来计算loss\n",
    "    labels = [-100] * len(user_tokens['input_ids']) + \\\n",
    "        input_ids[len(user_tokens['input_ids']):]\n",
    "    \n",
    "    # 长度截断\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\":input_ids,\n",
    "        \"attention_mask\":attention_mask,\n",
    "        \"labels\":labels\n",
    "    }\n",
    "    \n",
    "\n",
    "# 转化数据,删除原始列名，默认是保留的(instruction,input,output)\n",
    "train_ds = ds.map(process_func,remove_columns = ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37dfa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n保持健康的三个提示。<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。<|im_end|>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer.decode(train_ds[0]['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
